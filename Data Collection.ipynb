{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998a5b4a-efa6-4694-b443-98c1f3173ac9",
   "metadata": {},
   "source": [
    "# Data Collection and Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907d7ea3-d6df-46ad-bcec-2a2bd7d9cbf4",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, we will discuss the data collection and preprocessing code, along with the steps that have been implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c19f6-f406-4447-a81b-52b57c4dd334",
   "metadata": {},
   "source": [
    "## Code to Access Genius API to get lyrics of Songs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d31f9-4b45-4788-9ce7-19dac7888235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lyricsgenius\n",
    "import time\n",
    "\n",
    "file_path = \"Your File Path \"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "GENIUS_ACCESS_TOKEN = \"Your Access token from genius \"\n",
    "genius = lyricsgenius.Genius(GENIUS_ACCESS_TOKEN)\n",
    "\n",
    "\n",
    "df[\"lyrics\"] = None\n",
    "\n",
    "def fetch_lyrics(song_name):\n",
    "    try:\n",
    "        song = genius.search_song(song_name)\n",
    "        if song:\n",
    "            return song.lyrics\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching lyrics for {song_name}: {e}\")\n",
    "    return None\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    song_name = row[\"song\"]  \n",
    "    print(f\"üéµ Fetching lyrics for: {song_name}...\")\n",
    "    \n",
    "    df.at[index, \"lyrics\"] = fetch_lyrics(song_name)\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "output_file = \"songs_with_lyrics.csv\"\n",
    "df.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"‚úÖ Lyrics saved to `{output_file}`!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110eb5ef-6fa0-4a2c-8bd0-b97dfefdf7f9",
   "metadata": {},
   "source": [
    "## Code to merge all datasets, Perform Sentiment Analysis and drop null and duplicate values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff9bf976-d238-48f4-a1c7-1da7225f2ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/sanro/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 'popularity' column exists!\n",
      "‚úÖ Final cleaned dataset saved as 'cleaned_combine.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from scipy import stats\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "spotify_2018 = pd.read_csv(\"SpotifyAudioFeaturesNov2018.csv\")\n",
    "spotify_2019 = pd.read_csv(\"SpotifyAudioFeaturesApril2019.csv\")\n",
    "lyrics_df = pd.read_csv(\"spotify_millsongdata 2.csv\")\n",
    "genius_lyrics_df = pd.read_csv(\"songs_with_lyrics.csv\")\n",
    "\n",
    "spotify_df = pd.concat([spotify_2018, spotify_2019], ignore_index=True)\n",
    "\n",
    "spotify_df.rename(columns={\"track_name\": \"song\"}, inplace=True)\n",
    "\n",
    "for df in [spotify_df, lyrics_df, genius_lyrics_df]:\n",
    "    df[\"song\"] = df[\"song\"].str.lower().str.strip()\n",
    "\n",
    "all_lyrics = pd.concat([lyrics_df, genius_lyrics_df], ignore_index=True)\n",
    "\n",
    "merged_df = pd.merge(spotify_df, all_lyrics, on=\"song\", how=\"left\")\n",
    "\n",
    "merged_df.drop_duplicates(subset=[\"song\", \"artist_name\"], keep=\"first\", inplace=True)\n",
    "\n",
    "for col in merged_df.columns:\n",
    "    if col.endswith(\"_x\"):\n",
    "        base_col = col[:-2]  # Remove '_x' suffix\n",
    "        if base_col + \"_y\" in merged_df.columns:\n",
    "            merged_df[base_col] = merged_df[col]  \n",
    "            merged_df.drop(columns=[col, base_col + \"_y\"], inplace=True)\n",
    "\n",
    "if \"popularity\" in merged_df.columns:\n",
    "    print(\"‚úÖ 'popularity' column exists!\")\n",
    "else:\n",
    "    print(\"‚ùå 'popularity' column is missing!\")\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(lyrics):\n",
    "    if pd.isna(lyrics) or lyrics.strip() == \"\":\n",
    "        return pd.Series([0, 0, 0, 0, 0, 0, 1])  \n",
    "    \n",
    "    sentiment_scores = sia.polarity_scores(lyrics)\n",
    "    compound = sentiment_scores[\"compound\"]\n",
    "    pos = sentiment_scores[\"pos\"]\n",
    "    neu = sentiment_scores[\"neu\"]\n",
    "    neg = sentiment_scores[\"neg\"]\n",
    "\n",
    "    textblob_analysis = TextBlob(lyrics)\n",
    "    sentiment_polarity = textblob_analysis.sentiment.polarity  \n",
    "    subjectivity = textblob_analysis.sentiment.subjectivity  \n",
    "\n",
    "    not_positive = 1 if pos < 0.2 else 0  \n",
    "\n",
    "    return pd.Series([neg, neu, pos, compound, sentiment_polarity, subjectivity, not_positive])\n",
    "\n",
    "merged_df[['Negative', 'Neutral', 'Positive', 'Compound', 'TextBlob_Polarity', 'TextBlob_Subjectivity', 'Not_Positive']] = merged_df['text'].apply(analyze_sentiment)\n",
    "\n",
    "numeric_columns = merged_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "z_scores = stats.zscore(merged_df[numeric_columns])\n",
    "outliers = (np.abs(z_scores) > 3).any(axis=1)  \n",
    "df_cleaned = merged_df[~outliers]  \n",
    "\n",
    "def categorize_popularity(value):\n",
    "    if value <= 30:\n",
    "        return 0  # Low Popularity\n",
    "    elif value <= 70:\n",
    "        return 1  # Medium Popularity\n",
    "    else:\n",
    "        return 2  # High Popularity\n",
    "        \n",
    "df_cleaned = df_cleaned.copy()  \n",
    "df_cleaned[\"Popularity_Label\"] = df_cleaned[\"popularity\"].apply(categorize_popularity)\n",
    "\n",
    "selected_features = [\n",
    "    \"duration_ms\", \"popularity\", \"danceability\", \"energy\", \"key\", \"loudness\",\n",
    "    \"Compound\", \"TextBlob_Polarity\", \"TextBlob_Subjectivity\", \n",
    "    \"Negative\", \"Neutral\", \"Positive\", \"Not_Positive\"\n",
    "]\n",
    "\n",
    "df_cleaned = df_cleaned.dropna(subset=selected_features)\n",
    "\n",
    "df_cleaned.to_csv(\"cleaned_combine.csv\", index=False)\n",
    "print(f\"‚úÖ Final cleaned dataset saved as 'cleaned_combine.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496579d8-b2f0-4be4-af23-8bf4b985217d",
   "metadata": {},
   "source": [
    "Code to see the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06fa8630-fae3-4c39-b7ec-560b51ef78f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  artist_name                track_id  \\\n",
      "0          YG  2RM4jf1Xa9zPgMGRDiht8O   \n",
      "1          YG  1tHDG53xJNGsItRA3vfVgs   \n",
      "2       R3HAB  6Wosx2euFPMT14UXiWudMy   \n",
      "3  Chris Cooq  3J2Jpw61sO7l6Hc7qdYV91   \n",
      "4  Chris Cooq  2jbYvQCyPgX3CdmAzeVeuS   \n",
      "\n",
      "                                             song  time_signature artist link  \\\n",
      "0  big bank feat. 2 chainz, big sean, nicki minaj               4    NaN  NaN   \n",
      "1                    band drum (feat. a$ap rocky)               4    NaN  NaN   \n",
      "2                                   radio silence               4    NaN  NaN   \n",
      "3                                         lactose               4    NaN  NaN   \n",
      "4                             same - original mix               4    NaN  NaN   \n",
      "\n",
      "  text explicit  year genre  ... valence  popularity  Negative  Neutral  \\\n",
      "0  NaN      NaN   NaN   NaN  ...   0.118          44       0.0      0.0   \n",
      "1  NaN      NaN   NaN   NaN  ...   0.371          10       0.0      0.0   \n",
      "2  NaN      NaN   NaN   NaN  ...   0.382          63       0.0      0.0   \n",
      "3  NaN      NaN   NaN   NaN  ...   0.641           9       0.0      0.0   \n",
      "4  NaN      NaN   NaN   NaN  ...   0.928           8       0.0      0.0   \n",
      "\n",
      "   Positive  Compound  TextBlob_Polarity  TextBlob_Subjectivity  Not_Positive  \\\n",
      "0       0.0       0.0                0.0                    0.0           1.0   \n",
      "1       0.0       0.0                0.0                    0.0           1.0   \n",
      "2       0.0       0.0                0.0                    0.0           1.0   \n",
      "3       0.0       0.0                0.0                    0.0           1.0   \n",
      "4       0.0       0.0                0.0                    0.0           1.0   \n",
      "\n",
      "   Popularity_Label  \n",
      "0                 1  \n",
      "1                 0  \n",
      "2                 1  \n",
      "3                 0  \n",
      "4                 0  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_combine.csv\")\n",
    "print(df.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e1a17-91b4-404d-a609-fe121ded84ed",
   "metadata": {},
   "source": [
    "## Code to Split the Data set into Training, Testing and Validation and also applying Smote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d71fe65-0833-46c5-877a-9e5821d3e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Class Distribution:\n",
      " Popularity_Label\n",
      "0    55973\n",
      "1    29964\n",
      "2     1010\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Testing Class Distribution:\n",
      " Popularity_Label\n",
      "0    13994\n",
      "1     7491\n",
      "2      252\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data Split Completed! Training and Testing files saved.\n",
      "Training Class Distribution (after second split):\n",
      " Popularity_Label\n",
      "0    44778\n",
      "1    23971\n",
      "2      808\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation Class Distribution:\n",
      " Popularity_Label\n",
      "0    11195\n",
      "1     5993\n",
      "2      202\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Second Data Split Completed! Final Training and Validation files saved.\n",
      "\n",
      " SMOTE Applied & Data Standardized.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "file_path = \"cleaned_combine.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "selected_features = [\n",
    "    'duration_ms', 'popularity', 'danceability', 'energy', 'key', 'loudness',\n",
    "    'Compound', 'TextBlob_Polarity', 'TextBlob_Subjectivity', \n",
    "    'Negative', 'Neutral', 'Positive', 'Not_Positive'\n",
    "]\n",
    "\n",
    "df_cleaned = df.dropna(subset=selected_features)\n",
    "\n",
    "# Convert Popularity into categories (Low, Medium, High)\n",
    "def categorize_popularity(value):\n",
    "    if value <= 30:\n",
    "        return 0  # Low Popularity\n",
    "    elif value <= 70:\n",
    "        return 1  # Medium Popularity\n",
    "    else:\n",
    "        return 2  # High Popularity\n",
    "\n",
    "df_cleaned[\"Popularity_Label\"] = df_cleaned[\"popularity\"].apply(categorize_popularity)\n",
    "\n",
    "X = df_cleaned[selected_features].drop(columns=[\"popularity\"], errors=\"ignore\")\n",
    "y = df_cleaned[\"Popularity_Label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training Class Distribution:\\n\", y_train.value_counts())\n",
    "print(\"\\nTesting Class Distribution:\\n\", y_test.value_counts())\n",
    "\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "train_data.to_csv(\"training_data.csv\", index=False)\n",
    "test_data.to_csv(\"testing_data.csv\", index=False)\n",
    "\n",
    "print(\"\\nData Split Completed! Training and Testing files saved.\")\n",
    "\n",
    "train_file_path = \"training_data.csv\"\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "\n",
    "X_train_full = train_df.drop(columns=[\"Popularity_Label\"], errors=\"ignore\")\n",
    "y_train_full = train_df[\"Popularity_Label\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n",
    ")\n",
    "\n",
    "print(\"Training Class Distribution (after second split):\\n\", y_train.value_counts())\n",
    "print(\"\\nValidation Class Distribution:\\n\", y_val.value_counts())\n",
    "\n",
    "train_data_final = pd.concat([X_train, y_train], axis=1)\n",
    "val_data = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "train_data_final.to_csv(\"final_training_data.csv\", index=False)\n",
    "val_data.to_csv(\"validation_data.csv\", index=False)\n",
    "\n",
    "print(\"\\n Second Data Split Completed! Final Training and Validation files saved.\")\n",
    "\n",
    "train_file = \"final_training_data.csv\"\n",
    "val_file = \"validation_data.csv\"\n",
    "test_file = \"testing_data.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_file)\n",
    "val_df = pd.read_csv(val_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "X_train = train_df.drop(columns=[\"Popularity_Label\"], errors=\"ignore\")\n",
    "y_train = train_df[\"Popularity_Label\"]\n",
    "X_val = val_df.drop(columns=[\"Popularity_Label\"], errors=\"ignore\")\n",
    "y_val = val_df[\"Popularity_Label\"]\n",
    "X_test = test_df.drop(columns=[\"Popularity_Label\"], errors=\"ignore\")\n",
    "y_test = test_df[\"Popularity_Label\"]\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n SMOTE Applied & Data Standardized.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
